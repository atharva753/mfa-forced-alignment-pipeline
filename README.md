# Montreal Forced Aligner (MFA) Pipeline for Speech Analysis

## Overview

This repository contains a complete automated pipeline for phonetic forced alignment using the Montreal Forced Aligner (MFA). The system processes speech audio files and their corresponding transcriptions to generate precise word-level and phoneme-level time alignments, which are essential for phonetic research, speech synthesis, and acoustic analysis.

## Project Objectives

The primary goals of this project include:

- Implementing end-to-end forced alignment workflow for speech data
- Understanding the mechanics of automatic alignment between audio and phonetic transcription
- Extracting detailed acoustic measurements including formants, pitch, and intensity
- Analyzing alignment quality and identifying potential errors
- Automating the complete pipeline for batch processing
- Training custom grapheme-to-phoneme models for handling out-of-vocabulary words

## Repository Contents

### Scripts

**mfa_automation.py**: Complete automation script that handles corpus validation, forced alignment execution, and TextGrid generation. This script manages the entire MFA workflow from input validation through output generation.

**acoustic_analysis.py**: Comprehensive acoustic measurement extraction tool that processes TextGrid files to extract phoneme durations, vowel formants (F1, F2, F3), pitch statistics (F0), and intensity measurements. Requires the Parselmouth library for Praat integration.

**alignment_quality_checker.py**: Quality assurance tool that identifies potential alignment errors including duration anomalies, timing gaps, statistical outliers, and word-phoneme consistency issues.

### Data

The sample corpus includes six audio files with corresponding transcription files. Three files contain broadcast news speech (F2BJRLP series) and three contain controlled minimal pair recordings (ISLE corpus series) designed to test phonetic contrasts.

### Outputs

TextGrid files generated by MFA contain two tiers: word-level annotations showing precise word boundaries, and phone-level annotations marking individual phoneme segments. The measurements directory contains comprehensive CSV files with acoustic data for all phonemes and words, along with summary statistics in JSON format. Quality reports provide detailed analysis of alignment accuracy and potential issues.

## System Requirements

### Software Dependencies

- Python 3.11 or higher
- Anaconda or Miniconda package manager
- Montreal Forced Aligner 3.3.8
- Praat (for manual inspection and visualization)

### Python Libraries

The required libraries include pandas for data manipulation, numpy for numerical computations, parselmouth for Praat integration, and the standard json library for report generation.

## Installation Instructions

### Step 1: Install Miniconda

For Windows systems, download Miniconda from the official repository. Open Command Prompt and execute the installation command. After installation completes, close and reopen Command Prompt to ensure the conda command is recognized.

```bash
curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe --output Miniconda3-latest-Windows-x86_64.exe
```

Follow the installation wizard. When prompted about adding to PATH, you may skip this step as we will use Anaconda Prompt instead.

### Step 2: Create Conda Environment

Open Anaconda Prompt and create an isolated environment specifically for MFA work. This ensures dependencies do not conflict with other projects.

```bash
conda create -n mfa python=3.11
```

When prompted to proceed, confirm by typing 'y'. This process creates a clean environment with Python 3.11.

### Step 3: Activate Environment

Activate the newly created environment. You will notice the prompt changes from (base) to (mfa), indicating the environment switch.

```bash
conda activate mfa
```

### Step 4: Install Montreal Forced Aligner

Install MFA and all its dependencies from the conda-forge channel. This installation includes Kaldi for acoustic modeling, various audio processing libraries, and the core MFA tools.

```bash
conda install -c conda-forge montreal-forced-aligner
```

This installation requires approximately 360 MB and may take five to ten minutes depending on your connection speed.

### Step 5: Verify Installation

Confirm that MFA installed correctly by checking the version number.

```bash
mfa version
```

You should see version 3.3.8 displayed.

### Step 6: Download Pre-trained Models

Download the standard American English dictionary and acoustic model. These models were trained on hundreds of hours of speech data and provide high-quality alignments for general American English.

```bash
mfa model download dictionary english_us_arpa
mfa model download acoustic english_us_arpa
```

### Step 7: Install Python Dependencies

Install the additional Python libraries required for acoustic analysis and quality checking.

```bash
pip install pandas numpy praat-parselmouth
```

## Dataset Preparation

### Corpus Structure

MFA requires a specific folder structure where each audio file has a matching transcript file with the same base name. The corpus directory should contain pairs of files: for example, audio_01.wav paired with audio_01.txt, audio_02.wav paired with audio_02.txt, and so forth.

### File Requirements

Audio files must be in WAV format with 16kHz or 22kHz sampling rate recommended. Mono channel audio is preferred, though stereo files are accepted. Transcript files should be plain text (TXT format) using UTF-8 encoding. Each transcript should contain only the spoken words without timestamps, speaker labels, or other annotations. Punctuation may be included but is optional.

### Example Corpus Structure

```
mfa_corpus/
├── F2BJRLP1.wav
├── F2BJRLP1.txt
├── F2BJRLP2.wav
├── F2BJRLP2.txt
└── ...
```

## Usage Instructions

### Running the Complete Pipeline

The automation script handles the entire workflow from validation through TextGrid generation. Before running, update the paths in the script to match your directory structure.

```bash
python scripts/mfa_automation.py
```

The script performs corpus validation to check for out-of-vocabulary words and file format issues, executes forced alignment using the specified dictionary and acoustic model, exports TextGrid files with word and phone annotations, and generates a comprehensive JSON report with alignment statistics.

### Manual MFA Commands

For granular control, you may execute individual MFA commands directly.

**Validate Corpus**: Check for potential issues before alignment.

```bash
mfa validate path/to/corpus english_us_arpa
```

**Run Alignment**: Perform forced alignment on your corpus.

```bash
mfa align path/to/corpus english_us_arpa english_us_arpa path/to/output
```

**Train G2P Model**: Create a custom grapheme-to-phoneme model for handling unknown words.

```bash
mfa train_g2p english_us_arpa my_g2p_model
```

### Extracting Acoustic Measurements

The acoustic analysis script processes all TextGrid files and extracts detailed measurements. Update the folder paths in the script before execution.

```bash
python scripts/acoustic_analysis.py
```

This generates phoneme_measurements.csv containing duration, formants, pitch, and intensity for each phoneme, word_measurements.csv with word-level duration statistics, and analysis_summary.json with aggregate statistics and formant ranges.

### Checking Alignment Quality

The quality checker identifies potential alignment errors and provides detailed diagnostics.

```bash
python scripts/alignment_quality_checker.py
```

The output includes counts of duration anomalies (too short or too long phonemes), timing gaps and overlaps between consecutive segments, statistical outliers (phonemes deviating significantly from expected durations), and word-phoneme consistency checks.

## Interpreting Results

### TextGrid Structure

Each TextGrid file contains two interval tiers. The words tier shows word boundaries with labels indicating the transcribed words. The phones tier displays phoneme boundaries using ARPA notation, such as AH0 for unstressed schwa, EY1 for primary-stressed "ay" vowel, and K for unvoiced velar stop.

### Visualization in Praat

To examine alignments visually, open Praat and load both the WAV file and its corresponding TextGrid file. Select both objects in the Praat Objects window and click "View & Edit" to display the waveform, spectrogram, and annotation tiers simultaneously. This allows verification of boundary placement accuracy.

### Common Phoneme Symbols (ARPA)

The ARPA phoneme set uses ASCII characters for ease of processing. Vowels include AA (as in "father"), AE (as in "cat"), AH (as in "but"), EH (as in "bed"), IH (as in "bit"), and IY (as in "beat"). Consonants include B (voiced bilabial stop), D (voiced alveolar stop), K (voiceless velar stop), S (voiceless alveolar fricative), and T (voiceless alveolar stop). Numbers indicate stress levels: 0 for unstressed, 1 for primary stress, and 2 for secondary stress.

## Quality Metrics

### Alignment Accuracy

The alignment quality for this project achieved a six percent error rate, which is considered good for forced alignment systems. Most identified issues represented natural speech variation rather than true alignment errors.

### Common Issues

Short unstressed vowels often appear in rapid speech where vowel reduction naturally occurs. Filled pauses marked as "spn" (spoken noise) represent hesitations such as "uh" or "um" and are correctly identified as long segments. Timing gaps between phonemes typically indicate natural pauses, breath intake points, or phrase boundaries.

### Statistical Findings

Analysis of the sample corpus revealed 1,022 phonemes with a mean duration of 79 milliseconds and 241 words with a mean duration of 335 milliseconds. Among 397 analyzed vowels, the average first formant measured 506 Hz and the average second formant measured 1771 Hz, indicating a balanced vowel space typical of American English.

## Training Custom Models

### When to Train G2P Models

Custom grapheme-to-phoneme models become necessary when working with specialized vocabulary not present in standard dictionaries, such as technical terminology, proper names and locations, brand names and neologisms, or non-standard spellings and dialectal variations.

### Training Process

The training command uses an existing dictionary as the learning source.

```bash
mfa train_g2p english_us_arpa my_custom_g2p
```

This process analyzes pronunciation patterns across thousands of word-phoneme pairs, learns grapheme-to-phoneme correspondence rules, and creates a model capable of predicting pronunciations for unseen words. Training duration depends on dictionary size but typically requires several hours.

## Troubleshooting

### Common Installation Issues

If conda is not recognized after Miniconda installation, close and reopen Command Prompt or use Anaconda Prompt instead of regular Command Prompt. If MFA installation fails due to dependency conflicts, create a fresh environment and avoid mixing conda and pip installations in the base environment.

### Runtime Errors

Out-of-vocabulary words can be handled by training a custom G2P model or manually adding pronunciations to a custom dictionary file. Memory errors during alignment of large files may be resolved by processing files in smaller batches or increasing the system's available RAM. TextGrid reading errors in Python scripts often stem from path issues, so verify that file paths use raw strings with the 'r' prefix.

### Performance Optimization

For large corpora, consider using the parallel processing flag to utilize multiple CPU cores, though this requires multiple speakers in the corpus. The num_jobs parameter can be adjusted based on available system resources.

## Project Results

### Sample Outputs

The repository includes six complete TextGrid files demonstrating alignment quality across different speech types. Broadcast news recordings show natural pauses, filled hesitations, and variable speech rate. Minimal pair recordings demonstrate precise phonetic contrasts with emphatic pronunciation.

### Key Observations

Word-level boundaries achieved perfect consistency with zero mismatches between word and phoneme duration sums. Phoneme-level alignment showed 94 percent accuracy with most flagged anomalies representing natural speech phenomena rather than errors. Duration measurements proved suitable for phonetic research after filtering statistical outliers. Formant extraction succeeded for 397 vowels with reliable F1 and F2 measurements. The automated pipeline reduced manual processing time from hours to minutes per file.

## References and Resources

The Montreal Forced Aligner documentation provides comprehensive technical details at https://montreal-forced-aligner.readthedocs.io. The MFA GitHub repository at https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner offers source code and issue tracking. Praat software for phonetic analysis is available at https://www.fon.hum.uva.nl/praat/. The ARPA phoneme set documentation explains American English phoneme notation. Research publications on forced alignment methodology provide theoretical background for the alignment algorithms.

## License

This project is provided for educational and research purposes. The Montreal Forced Aligner itself is distributed under the MIT License. Please cite the MFA project if you use this pipeline in published research.

## Author and Contact

This pipeline was developed as part of a speech research project focusing on automatic phonetic alignment and acoustic analysis. For questions or collaboration inquiries, please open an issue in this repository.

## Acknowledgments

This work builds upon the Montreal Forced Aligner developed by the Montreal Corpus Tools team. The sample data includes excerpts from the ISLE corpus and broadcast news recordings. Special thanks to the developers of Praat, Kaldi, and the Python scientific computing ecosystem for providing the foundational tools that make this research possible.
